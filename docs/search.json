[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbprocess",
    "section": "",
    "text": "Process and export Jupyter Notebooks fast\n\nThis file will become your README and also the index of your documentation.\n\n\nWith pip:\npip install nbprocess\nWith conda:\nconda install -c fastai nbprocess\n\n\n\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "09_filter.html",
    "href": "09_filter.html",
    "title": "nbprocess",
    "section": "",
    "text": "Filter for quarto processing\n\n\ncreate_quarto()\n\n\u001b[1m\u001b[34m[ 1/10] index.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 2/10] 09_filter.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 3/10] 08_processors.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 4/10] 06_merge.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 5/10] 03_process.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 6/10] 04_export.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 7/10] 02_maker.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 8/10] 07_clean.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[ 9/10] 01_read.ipynb\u001b[39m\u001b[22m\n\u001b[1m\u001b[34m[10/10] 05_sync.ipynb\u001b[39m\u001b[22m\n\nOutput created: docs/index.html\n\n\u001b[1mpandoc -o README.md\u001b[22m\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  standalone: true\n  default-image-extension: png\n  html-math-method: webtex\n  filters:\n    - crossref\n  \nOutput created: docs/README.md\n\n\n\n\n#skip\nfrom nbprocess.export import nbs_export\nnbs_export()"
  },
  {
    "objectID": "08_processors.html",
    "href": "08_processors.html",
    "title": "nbprocess",
    "section": "",
    "text": "Some processors for NBProcessor\n\n\nfrom fastcore.test import *\n\n\n_test_file = '../tests/docs_test.ipynb'\n\ndef _run_procs(procs=None, preprocs=None, postprocs=None):\n    nbp = NBProcessor(_test_file, procs, preprocs=preprocs, postprocs=postprocs)\n    nbp.process()\n    return '\\n'.join([str(cell) for cell in nbp.nb.cells])\n\nGets rid of colors that are streamed from standard out, which can interfere with static site generators:\n\nres = _run_procs(strip_ansi)\nassert not _re_ansi_escape.findall(res)\n\nTo inject metadata make a comment in a cell with the following pattern: #cell_meta:{key=value}. Note that #meta is an alias for #cell_meta\nFor example, at the moment, this notebook has no cells with metadata, which we can see b using show_meta:\n\n_run_procs([show_meta]);\n\nHowever, after we process this notebook with inject_meta, the appropriate metadata will be injected:\n\n_run_procs([meta_,show_meta]);\n\n{'show_steps': ('start', 'train')}\n\n\nThis preprocessor inserts a warning in the markdown destination that the file is autogenerated. This warning is inserted in the second cell so we do not interfere with front matter.\n\nres = _run_procs(preprocs=[insert_warning])\nassert \"<!-- WARNING: THIS FILE WAS AUTOGENERATED!\" in res\n\n\nres = _run_procs(remove_)\nassert 'you will not be able to see this cell at all either' not in res\nassert 'THE output is removed' not in res and 'output is removed' in res\nassert \"the','code\" not in res and 'code that created me' in res\n\n\nres = _run_procs(hide_line)\nassert r\"def show():\\n    a = 2\" in res\n\n\nres = _run_procs(filter_stream_)\nexp=r\"'A line\\n', 'Another line.\\n'\"\nassert exp in res\n\n\nres = _run_procs(clean_magics)\nassert \"%%\" not in res\n\nWhen we issue a shell command in a notebook with !, we need to change the code-fence from python to bash and remove the !:\n\nres = _run_procs(bash_identify)\nassert \"'echo\" in res\n\n\nres = _run_procs(rm_header_dash)\nassert 'some words' in res\nassert 'A heading to Hide' not in res and 'Another Heading' not in res and 'Yet another heading to hide' not in res\n\n\nres = _run_procs(rm_export)\nassert 'dontshow' not in res\n\n\nres = _run_procs(clean_show_doc)\nassert 'ShowDoc(1)' not in res\nassert 'showdoc output' in res\n\n\nfrom nbprocess.export import nbs_export\nnbs_export()"
  },
  {
    "objectID": "06_merge.html",
    "href": "06_merge.html",
    "title": "nbprocess",
    "section": "",
    "text": "Fix merge conflicts in jupyter notebooks\n\n\n\nWhen working with jupyter notebooks (which are json files behind the scenes) and GitHub, it is very common that a merge conflict (that will add new lines in the notebook source file) will break some notebooks you are working on. This module defines the function fix_conflicts to fix those notebooks for you, and attempt to automatically merge standard conflicts. The remaining ones will be delimited by markdown cells like this:\n<<<<<< HEAD\n\n# local code here\n\n======\n\n# remote code here\n\n>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\nBelow is an example of broken notebook. The json format is broken by the lines automatically added by git. Such a file can’t be opened in jupyter notebook.\n\nbroken = Path('../tests/example.ipynb.broken')\ntst_nb = broken.read_text()\nprint(tst_nb)\n\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"3\"\n      ]\n     },\n     \"execution_count\": 6,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n<<<<<<< HEAD\n    \"z=3\\n\",\n=======\n    \"z=2\\n\",\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n    \"z\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"6\"\n      ]\n     },\n<<<<<<< HEAD\n     \"execution_count\": 7,\n=======\n     \"execution_count\": 5,\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"x=3\\n\",\n    \"y=3\\n\",\n    \"x+y\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n\n\n\nNote that in this example, the second conflict is easily solved: it just concerns the execution count of the second cell and can be solved by choosing either option without really impacting your notebook. This is the kind of conflict we will fix automatically. The first conflict is more complicated as it spans across two cells and there is a cell present in one version, not the other. Such a conflict (and generally the ones where the inputs of the cells change form one version to the other) aren’t automatically fixed, but we will return a proper json file where the annotations introduced by git will be placed in markdown cells.\n\n\n\nThe approach we use is to first “unpatch” the conflicted file, regenerating the two files it was originally created from. Then we redo the diff process, but using cells instead of text lines.\nThe result of “unpatching” our conflicted test notebook is the two original notebooks it would have been created from. Each of these original notebooks will contain valid JSON:\n\na,b,branch1,branch2 = unpatch(tst_nb)\njson.loads(a)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=3\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 7,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\njson.loads(b)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=2\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 5,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\nbranch1,branch2\n\n('HEAD', 'a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35')\n\n\nThis begins by optionally backing the notebook fname to fname.bak in case something goes wrong. Then it parses the broken json, solving conflicts in cells. Every conflict that only involves metadata or outputs of cells will be solved automatically by using the local (theirs==False) or the remote (theirs==True) branch. Otherwise, or for conflicts involving the inputs of cells, the json will be repaired by including the two version of the conflicted cell(s) with markdown cells indicating the conflicts. You will be able to open the notebook again and search for the conflicts (look for <<<<<<<) then fix them as you wish.\nA message will be printed indicating whether the notebook was fully merged or if conflicts remain.\n\nfix_merge(broken, outname='tmp.ipynb')\nchk = read_nb('tmp.ipynb')\ntest_eq(len(chk.cells), 7)\n\nOne or more conflict remains in the notebook, please inspect manually."
  },
  {
    "objectID": "03_process.html",
    "href": "03_process.html",
    "title": "nbprocess",
    "section": "",
    "text": "Exporting a notebook to a library\n\n\nfrom fastcore.test import *\nfrom pdb import set_trace\nfrom importlib import reload\nimport shutil\n\nSpecial comments at the start of a cell can be used to provide information to nbprocess about how to process a cell, so we need to be able to find the location of these comments.\n\nminimal = read_nb('../tests/minimal.ipynb')\n\nComment directives start with #, followed by whitespace delimited tokens, which extract_directives extracts from the start of a cell, up until a blank line or a line containing something other than comments. The extracted lines are removed from the source.\n\nexp  = AttrDict(source = \"\"\"#|export module\n#| hide\n1+2\n#bar\"\"\")\ntest_eq(extract_directives(exp), dict(export=['module'],hide=[]))\ntest_eq(exp.source, \"1+2\\n#bar\")\n\nCell processors can be callables (e.g regular functions), in which case they are called for every cell:\n\neverything_fn = '../tests/01_everything.ipynb'\n\ndef print_execs(cell):\n    if 'exec' in cell.source: print(cell.source)\n\nNBProcessor(everything_fn, print_execs).process()\n\nexec(\"o_y=1\")\nexec(\"p_y=1\")\n_all_ = [o_y, 'p_y']\n\n\nComment directives are put in a cell attribute directive_ as a dictionary keyed by directive name:\n\ndef printme_func(cell):\n    if 'printme' in cell.directives_: print(cell.directives_['printme'])\n\nNBProcessor(everything_fn, printme_func).process()\n\n['testing']\n\n\nHowever, a more convenient way to handle comment directives is to use a class as a processor, and include a method in your class with the same name as your directive, surrounded by underscores:\n\nclass _PrintExample:\n    def _printme_(self, nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, _PrintExample()).process()\n\ntesting\n\n\nIn the case that your processor supports just one comment directive, you can just use a regular function, with the same name as your directive, but with an underscore appended – here printme_ is identical to _PrintExample above:\n\ndef printme_(nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, printme_).process()\n\ntesting\n\n\n\nbasic_export_nb2('00_read.ipynb', 'read')\nbasic_export_nb2('01_maker.ipynb', 'maker')\nbasic_export_nb2('02_process.ipynb', 'process')\n\ng = exec_new('import nbprocess.process')\nassert hasattr(g['nbprocess'].process, 'NBProcessor')"
  },
  {
    "objectID": "04_export.html",
    "href": "04_export.html",
    "title": "nbprocess",
    "section": "",
    "text": "Exporting a notebook to a library\n\n\nfrom fastcore.test import *\nfrom pdb import set_trace\nfrom importlib import reload\nimport shutil\n\nSpecify dest where the module(s) will be exported to, and optionally a class to use to create the module (ModuleMaker, by default).\nExported cells are stored in a dict called modules, where the keys are the modules exported to. Those without an explicit module are stored in the '#' key, which will be exported to default_exp.\n\neverything_fn = '../tests/01_everything.ipynb'\n\nexp = ExportModuleProc()\nproc = NBProcessor(everything_fn, exp)\nproc.process()\ntest_eq(exp.default_exp, 'everything')\nassert 'print_function'  in exp.modules['#'][0].source\nassert 'h_n' in exp.in_all['some.thing'][0].source\n\nLet’s check we can import a test file:\n\nshutil.rmtree('tmp', ignore_errors=True)\ncreate_modules('../tests/00_some.thing.ipynb', 'tmp')\n\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a'])\ntest_eq(g['tmp'].some.thing.a, 1)\n\nWe’ll also check that our ‘everything’ file exports correctly:\n\ncreate_modules(everything_fn, 'tmp')\n\ng = exec_new('import tmp.everything; from tmp.everything import *')\n_alls = L(\"a b d e m n o p q\".split())\nfor s in _alls.map(\"{}_y\"): assert s in g, s\nfor s in \"c_y_nall _f_y_nall g_n h_n i_n j_n k_n l_n\".split(): assert s not in g, s\nfor s in _alls.map(\"{}_y\") + [\"c_y_nall\", \"_f_y_nall\"]: assert hasattr(g['tmp'].everything,s), s\n\nThat notebook should also export one extra function to tmp.some.thing:\n\ndel(sys.modules['tmp.some.thing']) # remove from module cache\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a','h_n'])\ntest_eq(g['tmp'].some.thing.h_n(), None)\n\n\nPath('../nbprocess/export.py').unlink(missing_ok=True)\nnbs_export()\n\ng = exec_new('import nbprocess.export')\nassert hasattr(g['nbprocess'].export, 'nb_export')"
  },
  {
    "objectID": "02_maker.html",
    "href": "02_maker.html",
    "title": "nbprocess",
    "section": "",
    "text": "Create one or more modules from selected notebook cells\n\n\nfrom fastcore.test import *\nfrom pdb import set_trace\nfrom importlib import reload\n\n\n\nThese functions let us find and modify the definitions of variables in python modules.\n\nt = '''a_=(1,\n  2,\n  3)\n\nb_=3'''\ntest_eq(find_var(t.splitlines(), 'a_'), (0,3))\ntest_eq(find_var(t.splitlines(), 'b_'), (4,5))\n\n\ntest_eq(read_var(t, 'a_'), (1,2,3))\ntest_eq(read_var(t, 'b_'), 3)\n\n\ng = exec_new(t)\ntest_eq((g['a_'],g['b_']), ((1,2,3),3))\nt2 = update_var('a_', lambda o:0, code=t)\nexec(t2, g)\ntest_eq((g['a_'],g['b_']), (0,3))\nt3 = update_var('b_', lambda o:0, code=t)\nexec(t3, g)\ntest_eq((g['a_'],g['b_']), ((1,2,3),0))\n\nIn order to export a notebook, we need an way to create a Python file. ModuleMaker fills that role. Pass in the directory where you want to module created, the name of the module, the path of the notebook source, and set is_new to True if this is a new file being created (rather than an existing file being added to). The location of the saved module will be in fname.\n\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=True)\nmm.fname\n\nPath('tmp/test/testing.py')\n\n\nWe want to add an __all__ to the top of the exported module. This methods autogenerates it from all code in cells.\n\nnb = make_code_cells(\"from __future__ import print_function\", \"def a():...\", \"def b():...\",\n                      \"c=d=1\", \"_f=1\", \"_g=1\", \"_all_=['_g']\", \"@patch\\ndef h(self:ca):...\")\ntest_eq(set(mm.make_all(nb)), set(['a','b','c','d', '_g']))\n\n\ntest_eq(relative_import('nbprocess.core', \"xyz\"), 'nbprocess.core')\ntest_eq(relative_import('nbprocess.core', 'nbprocess'), '.core')\n_p = Path('fastai')\ntest_eq(relative_import('fastai.core', _p/'vision'), '..core')\ntest_eq(relative_import('fastai.core', _p/'vision/transform'), '...core')\ntest_eq(relative_import('fastai.vision.transform', _p/'vision'), '.transform')\ntest_eq(relative_import('fastai.notebook.core', _p/'data'), '..notebook.core')\ntest_eq(relative_import('fastai.vision', _p/'vision'), '.')\n\n\nss = \"from nbprocess.export import *\\nfrom nbprocess.a.b import *\"\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess')\ntest_eq(cell.source, 'from .export import *\\nfrom .a.b import *')\n\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess/a')\ntest_eq(cell.source, 'from ..export import *\\nfrom .b import *')\n\n\ndef _print_file(fname, mx=None): print(Path(fname).read_text().strip()[:ifnone(mx,9999)])\n\n\ncells = make_code_cells(\"from __future__ import print_function\", \"#|export\\ndef a(): ...\", \"def b(): ...\")\nmm.make(cells, L([cells[1]]))\nprint(Path('tmp/test/testing.py').read_text())\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../01_export.ipynb.\n\n# %% ../01_export.ipynb 0\nfrom __future__ import print_function\n\n# %% auto 0\n__all__ = ['a']\n\n# %% ../01_export.ipynb 2\n#|export\ndef a(): ...\n\n# %% ../01_export.ipynb 3\ndef b(): ...\n\n\n\nPass all_cells=[] if you don’t want any __all__ added.\nIf is_new=False then the additional definitions are added to the bottom, and any existing __all__ is updated with the newly-added symbols.\n\nc2 = make_code_cells(\"def c(): ...\", \"def d(): ...\")\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=False)\nmm.make(c2, c2)\n\n\ng = exec_new('from tmp.test.testing import *')\nfor s in \"a c d\".split(): assert s in g, s\nassert 'b' not in g\nassert g['a']() is None\n\n\npath = Path('../nbprocess')\n(path/'read.py').unlink(missing_ok=True)\n(path/'maker.py').unlink(missing_ok=True)\n\nadd_init(path)\ncfg = get_config()\n\n\nbasic_export_nb2('00_read.ipynb', 'read')\nbasic_export_nb2('01_maker.ipynb', 'maker')\n\n\ng = exec_new('from nbprocess import maker')\nassert g['maker'].ModuleMaker\nassert 'ModuleMaker' in g['maker'].__all__"
  },
  {
    "objectID": "07_clean.html",
    "href": "07_clean.html",
    "title": "nbprocess",
    "section": "",
    "text": "from nbprocess.mkdocplug import YourPlugin"
  },
  {
    "objectID": "07_clean.html#utils",
    "href": "07_clean.html#utils",
    "title": "nbprocess",
    "section": "Utils",
    "text": "Utils\n\ntst = {'cell_type': 'code', 'execution_count': 26,\n       'metadata': {'hide_input': True, 'meta': 23},\n       'outputs': [{'execution_count': 2,\n                    'data': {\n                        'application/vnd.google.colaboratory.intrinsic+json': {'type': 'string'},\n                        'plain/text': ['sample output',]\n                    }, 'output': 'super'}],\n       'source': 'awesome_code'}\nnb = {'metadata': {'kernelspec': 'some_spec', 'jekyll': 'some_meta', 'meta': 37}, 'cells': [tst]}\n\nclean_nb(nb)\ntest_eq(nb['cells'][0], {'cell_type': 'code', 'execution_count': None,\n              'metadata': {'hide_input': True},\n              'outputs': [{'execution_count': None, \n                           'data': { 'plain/text': ['sample output',]},\n                           'output': 'super'}],\n              'source': 'awesome_code'})\ntest_eq(nb['metadata'], {'kernelspec': 'some_spec', 'jekyll': 'some_meta'})"
  },
  {
    "objectID": "07_clean.html#main-function",
    "href": "07_clean.html#main-function",
    "title": "nbprocess",
    "section": "Main function",
    "text": "Main function\nBy default (fname left to None), the all the notebooks in lib_folder are cleaned. You can opt in to fully clean the notebook by removing every bit of metadata and the cell outputs by passing clear_all=True."
  },
  {
    "objectID": "01_read.html",
    "href": "01_read.html",
    "title": "nbprocess",
    "section": "",
    "text": "Reading a notebook, and initial bootstrapping for notebook exporting\n\n\nimport time,nbclient,tempfile\nfrom IPython.display import Markdown\n\n\n\nA notebook is just a json file:\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal_txt = minimal_fn.read_json()\n\n\ndisplay_json(minimal_txt)\n\n{ 'cells': [ {'cell_type': 'markdown', 'metadata': {}, 'source': ['# A minimal notebook']},\n             { 'cell_type': 'code',\n               'execution_count': 1,\n               'metadata': {},\n               'outputs': [{'data': {'text/plain': ['2']}, 'execution_count': 1, 'metadata': {}, 'output_type': 'execute_result'}],\n               'source': ['# Do some arithmetic\\n', '1+1']},\n             {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': []}],\n  'metadata': { 'kernelspec': {'display_name': 'Python 3 (ipykernel)', 'language': 'python', 'name': 'python3'},\n                'language_info': { 'codemirror_mode': {'name': 'ipython', 'version': 3},\n                                   'file_extension': '.py',\n                                   'mimetype': 'text/x-python',\n                                   'name': 'python',\n                                   'nbconvert_exporter': 'python',\n                                   'pygments_lexer': 'ipython3',\n                                   'version': '3.9.5'}},\n  'nbformat': 4,\n  'nbformat_minor': 4}\n\n\nThe important bit for us is the cells:\n\ndisplay_json(minimal_txt['cells'])\n\n[ {'cell_type': 'markdown', 'metadata': {}, 'source': ['# A minimal notebook']},\n  { 'cell_type': 'code',\n    'execution_count': 1,\n    'metadata': {},\n    'outputs': [{'data': {'text/plain': ['2']}, 'execution_count': 1, 'metadata': {}, 'output_type': 'execute_result'}],\n    'source': ['# Do some arithmetic\\n', '1+1']},\n  {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': []}]\n\n\nThe second cell here is a code cell, however it contains no outputs, because it hasn’t been executed yet. To execute a notebook, we first need to convert it into a format suitable for nbclient (which expects some dict keys to be available as attrs, and some available as regular dict keys). Normally, nbformat is used for this step, but it’s rather slow and inflexible, so we’ll write our own function based on fastcore’s handy dict2obj, which makes all keys available as both attrs and keys.\nWe use an AttrDict subclass which has some basic functionality for accessing notebook cells.\nWe can now convert our JSON into this nbclient-compatible format, which pretty prints the source code of cells in notebooks…\n\nminimal = dict2nb(minimal_txt)\nminimal.cells[1]\n\n# Do some arithmetic\n1+1\n\n\n…and is in a format compatible with nbclient to execute it:\n\nnbclient.execute(minimal);\n\nOne nice feature of the output of dict2nb is that we can still use it as a dict, so display_json still works as before. We can see that the cell has been executed, and the output added back to the nb:\n\ncell = minimal.cells[1]\ncell.outputs\n\n[{'output_type': 'execute_result',\n  'metadata': {},\n  'data': {'text/plain': '2'},\n  'execution_count': 1}]\n\n\nThe abstract syntax tree of source code cells is available in the parsed_ property:\n\ncell.parsed_(), cell.parsed_()[0].value.op\n\n([<ast.Expr at 0x7fc3b42618e0>], <ast.Add at 0x7fc3edda5850>)\n\n\nSince loading JSON and converting to an NB is something we’ll do a lot, we’ll create a shortcut function for it:\n\nminimal = read_nb(minimal_fn)\nprint(minimal.cells[0])\n\n{'cell_type': 'markdown', 'metadata': {}, 'source': '# A minimal notebook', 'idx_': 0}\n\n\n\n\n\nnbprocess uses a settings.ini file in the root of the project to store all configuration details. This file is in ConfigParser format, and can be read and written conveniently using fastcore’s Config class.\nThis is a wrapper for fastcore’s save_config_file which sets some nbprocess defaults. It is also installed as a CLI command.\nget_config searches for settings.ini in the current directory, and then in all parent directories, stopping when it is found.\n\nnbprocess_create_config('fastai', path='..', nbs_path='nbs', tst_flags='tst', cfg_name='test_settings.ini')\ncfg = get_config('test_settings.ini')\ntest_eq(cfg.lib_name, 'nbprocess')\ntest_eq(cfg.git_url, \"https://github.com/fastai/nbprocess/tree/master/\")\ncwd = Path.cwd()\ntest_eq(cfg.config_path, cwd.parent.absolute())\ntest_eq(cfg.path('lib_path'), cwd.parent/'nbprocess')\ntest_eq(cfg.path('nbs_path'), cwd)\ntest_eq(cfg.path('doc_path'), cwd.parent/'docs')\n\n\n\n\nPython modules require a __init.py__ file in all directories that are modules. We assume that all directories containing a python file (including in subdirectories of any depth) is a module, and therefore add a __init__.py to each.\n\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    (d/'a/b').mkdir(parents=True)\n    (d/'a/b/f.py').touch()\n    (d/'a/c').mkdir()\n    add_init(d)\n    assert not (d/'a/c'/_init).exists(), \"Should not add init to dir without py file\"\n    for e in [d, d/'a', d/'a/b']: assert (e/_init).exists(),f\"Missing init in {e}\"\n\nThis is a simple exporter with just enough functionality to correctly export this notebook, in order to bootstrap the creation of nbprocess itself.\n\npath = Path('../nbprocess')\n(path/'read.py').unlink(missing_ok=True)\n\nadd_init(path)\nbasic_export_nb(\"00_read.ipynb\", 'read.py')\n\ng = exec_new('from nbprocess import read')\nassert g['read'].add_init\nassert 'add_init' in g['read'].__all__"
  },
  {
    "objectID": "05_sync.html",
    "href": "05_sync.html",
    "title": "nbprocess",
    "section": "",
    "text": "Propagating small changes in the library back to notebooks\n\nThe library is primarily developed in notebooks so any big changes should be made there. But sometimes, it’s easier to fix small bugs or typos in the modules directly. nbprocess_update_lib is the function that will propagate those changes back to the corresponding notebooks. Note that you can’t create new cells or reorder cells with that functionality, so your corrections should remain limited.\nThis returns the exact same dict as is read from the notebook JSON.\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal = read_nb(minimal_fn)\n\nminimal_dict = minimal_fn.read_json()\nassert minimal_dict==nb2dict(minimal)\n\nThis returns the exact same string as saved by Jupyter.\n\ntmp = Path('tmp.ipynb')\ntry:\n    minimal_txt = minimal_fn.read_text()\n    write_nb(minimal, tmp)\n    assert minimal_txt==tmp.read_text()\nfinally: tmp.unlink()\n\n\ntest_eq(absolute_import('xyz', 'nbprocess', 0), 'xyz')\ntest_eq(absolute_import('', 'nbprocess', 1), 'nbprocess')\ntest_eq(absolute_import('core', 'nbprocess', 1), 'nbprocess.core')\ntest_eq(absolute_import('core', 'nbprocess/vision', 2), 'nbprocess.core')\ntest_eq(absolute_import('transform', 'nbprocess/vision', 1), 'nbprocess.vision.transform')\ntest_eq(absolute_import('notebook.core', 'nbprocess/data', 2), 'nbprocess.notebook.core')\n\n\nfrom nbprocess.export import nbs_export\nnbs_export()"
  }
]