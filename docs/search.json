[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbprocess",
    "section": "",
    "text": "Process and export Jupyter Notebooks fast\n\nThis will probably become v2 of nbdev in the near-ish future.\n\n\nWith pip:\npip install nbprocess\nWith conda:\nconda install -c fastai nbprocess\n\n\n\nBy default docs are exported for use with Quarto. To install Quarto on Ubuntu, run make install. See the Quarto docs for other platforms.\nThe following CLI tools are provided:\n\nnbprocess_create_config: Create settings.ini skeleton\nnbprocess_export: Export notebooks to Python modules\nnbprocess_update: Update Python modules from a notebook\nnbprocess_fix: Fix merge conflicts in notebooks\nnbprocess_filter: A filter for Quarto\nnbprocess_quarto: Create Quarto web site"
  },
  {
    "objectID": "08_showdoc.html",
    "href": "08_showdoc.html",
    "title": "nbprocess",
    "section": "",
    "text": "get_name(obj)\n\nGet the name of obj\n\n\n\n\n\n\n\n\n\nqual_name(obj)\n\nGet the qualified name of obj\n\n\n\n\n\n\n\n\n\nShowDocRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\n\n\n\nBasicMarkdownRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\n\n\n\nshow_doc(sym, disp=True, renderer=None)\n\n\n\n\n\n\n\n\n\n\nf(x: int = 1)\n\nfunc docstring\n\n\n\n\n\n\n\n\n\nBasicHtmlRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\n\nF(x: int = 1)class docstring"
  },
  {
    "objectID": "06_merge.html",
    "href": "06_merge.html",
    "title": "nbprocess",
    "section": "",
    "text": "Fix merge conflicts in jupyter notebooks\n\n\n\nWhen working with jupyter notebooks (which are json files behind the scenes) and GitHub, it is very common that a merge conflict (that will add new lines in the notebook source file) will break some notebooks you are working on. This module defines the function fix_conflicts to fix those notebooks for you, and attempt to automatically merge standard conflicts. The remaining ones will be delimited by markdown cells like this:\n<<<<<< HEAD\n\n# local code here\n\n======\n\n# remote code here\n\n>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\nBelow is an example of broken notebook. The json format is broken by the lines automatically added by git. Such a file can’t be opened in jupyter notebook.\n\nbroken = Path('../tests/example.ipynb.broken')\ntst_nb = broken.read_text()\nprint(tst_nb)\n\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"3\"\n      ]\n     },\n     \"execution_count\": 6,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n<<<<<<< HEAD\n    \"z=3\\n\",\n=======\n    \"z=2\\n\",\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n    \"z\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"6\"\n      ]\n     },\n<<<<<<< HEAD\n     \"execution_count\": 7,\n=======\n     \"execution_count\": 5,\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"x=3\\n\",\n    \"y=3\\n\",\n    \"x+y\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n\n\n\nNote that in this example, the second conflict is easily solved: it just concerns the execution count of the second cell and can be solved by choosing either option without really impacting your notebook. This is the kind of conflict we will fix automatically. The first conflict is more complicated as it spans across two cells and there is a cell present in one version, not the other. Such a conflict (and generally the ones where the inputs of the cells change form one version to the other) aren’t automatically fixed, but we will return a proper json file where the annotations introduced by git will be placed in markdown cells.\n\n\n\nThe approach we use is to first “unpatch” the conflicted file, regenerating the two files it was originally created from. Then we redo the diff process, but using cells instead of text lines.\n\n\n\n\n\n\nunpatch(s: str)\n\nTakes a string with conflict markers and returns the two original files, and their branch names\n\n\n\nThe result of “unpatching” our conflicted test notebook is the two original notebooks it would have been created from. Each of these original notebooks will contain valid JSON:\n\na,b,branch1,branch2 = unpatch(tst_nb)\njson.loads(a)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=3\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 7,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\njson.loads(b)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=2\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 5,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\nbranch1,branch2\n\n('HEAD', 'a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35')\n\n\n\n\n\n\n\n\nfix_merge(nbname: str, outname: str = None, nobackup: bool = True, theirs: bool = False, noprint: bool = False)\n\nCreate working notebook from conflicted notebook nbname\n\n\n\nThis begins by optionally backing the notebook fname to fname.bak in case something goes wrong. Then it parses the broken json, solving conflicts in cells. Every conflict that only involves metadata or outputs of cells will be solved automatically by using the local (theirs==False) or the remote (theirs==True) branch. Otherwise, or for conflicts involving the inputs of cells, the json will be repaired by including the two version of the conflicted cell(s) with markdown cells indicating the conflicts. You will be able to open the notebook again and search for the conflicts (look for <<<<<<<) then fix them as you wish.\nA message will be printed indicating whether the notebook was fully merged or if conflicts remain.\n\nfix_merge(broken, outname='tmp.ipynb')\nchk = read_nb('tmp.ipynb')\ntest_eq(len(chk.cells), 7)\n\nOne or more conflict remains in the notebook, please inspect manually."
  },
  {
    "objectID": "03_process.html",
    "href": "03_process.html",
    "title": "nbprocess",
    "section": "",
    "text": "A notebook processor\n\nSpecial comments at the start of a cell can be used to provide information to nbprocess about how to process a cell, so we need to be able to find the location of these comments.\n\nminimal = read_nb('../tests/minimal.ipynb')\n\n\n\n\n\n\n\nextract_directives(cell, remove=True)\n\nTake leading comment directives from lines of code in ss, remove #|, and split\n\n\n\nComment directives start with #, followed by whitespace delimited tokens, which extract_directives extracts from the start of a cell, up until a blank line or a line containing something other than comments. The extracted lines are removed from the source.\n\nexp  = AttrDict(source = \"\"\"#|export module\n#| hide\n1+2\n#bar\"\"\")\ntest_eq(extract_directives(exp), dict(export=['module'],hide=[]))\ntest_eq(exp.source, \"1+2\\n#bar\")\n\n\n\n\n\n\n\nopt_set(var, newval)\n\nnewval if newval else var\n\n\n\n\n\n\n\n\n\ninstantiate(x)\n\nInstantiate x if it’s a type\n\n\n\n\n\n\n\n\n\nNBProcessor(path=None, procs=None, preprocs=None, postprocs=None, nb=None, debug=False, rm_directives=True)\n\nProcess cells and nbdev comments in a notebook\n\n\n\nCell processors can be callables (e.g regular functions), in which case they are called for every cell:\n\neverything_fn = '../tests/01_everything.ipynb'\n\ndef print_execs(cell):\n    if 'exec' in cell.source: print(cell.source)\n\nNBProcessor(everything_fn, print_execs).process()\n\nexec(\"o_y=1\")\nexec(\"p_y=1\")\n_all_ = [o_y, 'p_y']\n\n\nComment directives are put in a cell attribute directive_ as a dictionary keyed by directive name:\n\ndef printme_func(cell):\n    if 'printme' in cell.directives_: print(cell.directives_['printme'])\n\nNBProcessor(everything_fn, printme_func).process()\n\n['testing']\n\n\nHowever, a more convenient way to handle comment directives is to use a class as a processor, and include a method in your class with the same name as your directive, surrounded by underscores:\n\nclass _PrintExample:\n    def _printme_(self, nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, _PrintExample()).process()\n\ntesting\n\n\nIn the case that your processor supports just one comment directive, you can just use a regular function, with the same name as your directive, but with an underscore appended – here printme_ is identical to _PrintExample above:\n\ndef printme_(nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, printme_).process()\n\ntesting\n\n\n\nbasic_export_nb2('01_read.ipynb', 'read')\nbasic_export_nb2('02_maker.ipynb', 'maker')\nbasic_export_nb2('03_process.ipynb', 'process')\n\ng = exec_new('import nbprocess.process')\nassert hasattr(g['nbprocess'].process, 'NBProcessor')"
  },
  {
    "objectID": "04_export.html",
    "href": "04_export.html",
    "title": "nbprocess",
    "section": "",
    "text": "Exporting a notebook to a library\n\n\n\n\n\n\n\nExportModuleProc()\n\nA processor which exports code to a module\n\n\n\nSpecify dest where the module(s) will be exported to, and optionally a class to use to create the module (ModuleMaker, by default).\nExported cells are stored in a dict called modules, where the keys are the modules exported to. Those without an explicit module are stored in the '#' key, which will be exported to default_exp.\n\neverything_fn = '../tests/01_everything.ipynb'\n\nexp = ExportModuleProc()\nproc = NBProcessor(everything_fn, exp)\nproc.process()\ntest_eq(exp.default_exp, 'everything')\nassert 'print_function'  in exp.modules['#'][0].source\nassert 'h_n' in exp.in_all['some.thing'][0].source\n\n\n\n\n\n\n\ncreate_modules(path, dest, procs=None, debug=False, mod_maker=<class 'nbprocess.maker.ModuleMaker'>)\n\nCreate module(s) from notebook\n\n\n\nLet’s check we can import a test file:\n\nshutil.rmtree('tmp', ignore_errors=True)\ncreate_modules('../tests/00_some.thing.ipynb', 'tmp')\n\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a'])\ntest_eq(g['tmp'].some.thing.a, 1)\n\nWe’ll also check that our ‘everything’ file exports correctly:\n\ncreate_modules(everything_fn, 'tmp')\n\ng = exec_new('import tmp.everything; from tmp.everything import *')\n_alls = L(\"a b d e m n o p q\".split())\nfor s in _alls.map(\"{}_y\"): assert s in g, s\nfor s in \"c_y_nall _f_y_nall g_n h_n i_n j_n k_n l_n\".split(): assert s not in g, s\nfor s in _alls.map(\"{}_y\") + [\"c_y_nall\", \"_f_y_nall\"]: assert hasattr(g['tmp'].everything,s), s\n\nThat notebook should also export one extra function to tmp.some.thing:\n\ndel(sys.modules['tmp.some.thing']) # remove from module cache\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a','h_n'])\ntest_eq(g['tmp'].some.thing.h_n(), None)\n\n\n\n\n\n\n\nnb_export(nbname, lib_path=None)\n\n\n\n\n\n\n\n\n\n\nnbs_export(path: str = None, recursive: bool = True, symlinks: bool = True, file_glob: str = '*.ipynb', file_re: str = None, folder_re: str = None, skip_file_glob: str = None, skip_file_re: str = None, skip_folder_re: str = '^[_.]')\n\n\n\n\n\nPath('../nbprocess/export.py').unlink(missing_ok=True)\nnbs_export()\n\ng = exec_new('import nbprocess.export')\nassert hasattr(g['nbprocess'].export, 'nb_export')"
  },
  {
    "objectID": "02_maker.html",
    "href": "02_maker.html",
    "title": "nbprocess",
    "section": "",
    "text": "Create one or more modules from selected notebook cells\n\n\n\nThese functions let us find and modify the definitions of variables in python modules.\n\n\n\n\n\n\nfind_var(lines, varname)\n\nFind the line numbers where varname is defined in lines\n\n\n\n\nt = '''a_=(1,\n  2,\n  3)\n\nb_=3'''\ntest_eq(find_var(t.splitlines(), 'a_'), (0,3))\ntest_eq(find_var(t.splitlines(), 'b_'), (4,5))\n\n\n\n\n\n\n\nread_var(code, varname)\n\nEval and return the value of varname defined in code\n\n\n\n\ntest_eq(read_var(t, 'a_'), (1,2,3))\ntest_eq(read_var(t, 'b_'), 3)\n\n\n\n\n\n\n\nupdate_var(varname, func, fn=None, code=None)\n\nUpdate the definition of varname in file fn, by calling func with the current definition\n\n\n\n\ng = exec_new(t)\ntest_eq((g['a_'],g['b_']), ((1,2,3),3))\nt2 = update_var('a_', lambda o:0, code=t)\nexec(t2, g)\ntest_eq((g['a_'],g['b_']), (0,3))\nt3 = update_var('b_', lambda o:0, code=t)\nexec(t3, g)\ntest_eq((g['a_'],g['b_']), ((1,2,3),0))\n\n\n\n\n\n\n\nModuleMaker(dest, name, nb_path, is_new=True)\n\nHelper class to create exported library from notebook source cells\n\n\n\nIn order to export a notebook, we need an way to create a Python file. ModuleMaker fills that role. Pass in the directory where you want to module created, the name of the module, the path of the notebook source, and set is_new to True if this is a new file being created (rather than an existing file being added to). The location of the saved module will be in fname.\n\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=True)\nmm.fname\n\nPath('tmp/test/testing.py')\n\n\n\n\n\n\n\n\nretr_exports(trees)\n\n\n\n\n\n\n\n\n\n\nshow_doc(sym, disp=True, renderer=None)\n\n\n\n\n\n\n\n\n\n\nmake_code_cells(*ss)\n\n\n\n\n\n\n\n\n\n\nmake_code_cell(code)\n\n\n\n\nWe want to add an __all__ to the top of the exported module. This methods autogenerates it from all code in cells.\n\nnb = make_code_cells(\"from __future__ import print_function\", \"def a():...\", \"def b():...\",\n                      \"c=d=1\", \"_f=1\", \"_g=1\", \"_all_=['_g']\", \"@patch\\ndef h(self:ca):...\")\ntest_eq(set(mm.make_all(nb)), set(['a','b','c','d', '_g']))\n\n\n\n\n\n\n\nrelative_import(name, fname, level=0)\n\nConvert a module name to a name relative to fname\n\n\n\n\ntest_eq(relative_import('nbprocess.core', \"xyz\"), 'nbprocess.core')\ntest_eq(relative_import('nbprocess.core', 'nbprocess'), '.core')\n_p = Path('fastai')\ntest_eq(relative_import('fastai.core', _p/'vision'), '..core')\ntest_eq(relative_import('fastai.core', _p/'vision/transform'), '...core')\ntest_eq(relative_import('fastai.vision.transform', _p/'vision'), '.transform')\ntest_eq(relative_import('fastai.notebook.core', _p/'data'), '..notebook.core')\ntest_eq(relative_import('fastai.vision', _p/'vision'), '.')\n\n\n\n\n\n\n\nshow_doc(sym, disp=True, renderer=None)\n\n\n\n\n\n\n\n\n\n\nupdate_import(source, tree, libname, f=<function relative_import at 0x7fb4063ae5e0>)\n\n\n\n\n\nss = \"from nbprocess.export import *\\nfrom nbprocess.a.b import *\"\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess')\ntest_eq(cell.source, 'from .export import *\\nfrom .a.b import *')\n\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess/a')\ntest_eq(cell.source, 'from ..export import *\\nfrom .b import *')\n\n\n\n\n\n\n\nshow_doc(sym, disp=True, renderer=None)\n\n\n\n\n\ndef _print_file(fname, mx=None): print(Path(fname).read_text().strip()[:ifnone(mx,9999)])\n\n\ncells = make_code_cells(\"from __future__ import print_function\", \"#|export\\ndef a(): ...\", \"def b(): ...\")\nmm.make(cells, L([cells[1]]))\nprint(Path('tmp/test/testing.py').read_text())\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../01_export.ipynb.\n\n# %% ../01_export.ipynb 0\nfrom __future__ import print_function\n\n# %% auto 0\n__all__ = ['a']\n\n# %% ../01_export.ipynb 2\n#|export\ndef a(): ...\n\n# %% ../01_export.ipynb 3\ndef b(): ...\n\n\n\nPass all_cells=[] if you don’t want any __all__ added.\nIf is_new=False then the additional definitions are added to the bottom, and any existing __all__ is updated with the newly-added symbols.\n\nc2 = make_code_cells(\"def c(): ...\", \"def d(): ...\")\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=False)\nmm.make(c2, c2)\n\n\ng = exec_new('from tmp.test.testing import *')\nfor s in \"a c d\".split(): assert s in g, s\nassert 'b' not in g\nassert g['a']() is None\n\n\npath = Path('../nbprocess')\n(path/'read.py').unlink(missing_ok=True)\n(path/'maker.py').unlink(missing_ok=True)\n\nadd_init(path)\ncfg = get_config()\n\n\n\n\n\n\n\nbasic_export_nb2(fname, name, dest=None)\n\nA basic exporter to bootstrap nbprocess using ModuleMaker\n\n\n\n\nbasic_export_nb2('01_read.ipynb', 'read')\nbasic_export_nb2('02_maker.ipynb', 'maker')\n\n\ng = exec_new('from nbprocess import maker')\nassert g['maker'].ModuleMaker\nassert 'ModuleMaker' in g['maker'].__all__"
  },
  {
    "objectID": "07_clean.html",
    "href": "07_clean.html",
    "title": "nbprocess",
    "section": "",
    "text": "Strip notebooks from superfluous metadata\n\nTo avoid pointless conflicts while working with jupyter notebooks (with different execution counts or cell metadata), it is recommended to clean the notebooks before committing anything (done automatically if you install the git hooks with nbdev_install_git_hooks). The following functions are used to do that.\n\n\n\n\n\n\n\n\nclean_nb(nb, clear_all=False)\n\nClean nb from superfluous metadata\n\n\n\n\ntst = {'cell_type': 'code', 'execution_count': 26,\n       'metadata': {'hide_input': True, 'meta': 23},\n       'outputs': [{'execution_count': 2,\n                    'data': {\n                        'application/vnd.google.colaboratory.intrinsic+json': {'type': 'string'},\n                        'plain/text': ['sample output',]\n                    }, 'output': 'super'}],\n       'source': 'awesome_code'}\nnb = {'metadata': {'kernelspec': 'some_spec', 'jekyll': 'some_meta', 'meta': 37}, 'cells': [tst]}\n\nclean_nb(nb)\ntest_eq(nb['cells'][0], {'cell_type': 'code', 'execution_count': None,\n              'metadata': {'hide_input': True},\n              'outputs': [{'execution_count': None, \n                           'data': { 'plain/text': ['sample output',]},\n                           'output': 'super'}],\n              'source': 'awesome_code'})\ntest_eq(nb['metadata'], {'kernelspec': 'some_spec', 'jekyll': 'some_meta'})\n\n\n\n\n\n\n\n\n\n\nnbdev_clean_nbs(fname: str = None, clear_all: <function bool_arg at 0x7fdd31bd5700> = False, read_stdin: <function bool_arg at 0x7fdd31bd5700> = False)\n\nClean all notebooks in fname to avoid merge conflicts\n\n\n\nBy default (fname left to None), the all the notebooks in lib_folder are cleaned. You can opt in to fully clean the notebook by removing every bit of metadata and the cell outputs by passing clear_all=True."
  },
  {
    "objectID": "10_filter.html",
    "href": "10_filter.html",
    "title": "nbprocess",
    "section": "",
    "text": "Filter for quarto processing\n\n\n\n\n\n\n\nFilterDefaults()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nfilter_nb(nb_txt: 'str' = None)\n\n\n\n\n\n\n\n\n\n\ncreate_quarto(path: 'str' = None, doc_path: 'str' = None, symlinks: 'bool' = False, file_glob: 'str' = '*.ipynb', file_re: 'str' = None, folder_re: 'str' = None, skip_file_glob: 'str' = None, skip_file_re: 'str' = None, skip_folder_re: 'str' = '^[_.]')\n\n\n\n\n\n# create_quarto()\n\n\n#skip\nfrom nbprocess.export import nbs_export\nnbs_export()"
  },
  {
    "objectID": "01_read.html",
    "href": "01_read.html",
    "title": "nbprocess",
    "section": "",
    "text": "Reading a notebook, and initial bootstrapping for notebook exporting\n\n\n\nA notebook is just a json file:\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal_txt = minimal_fn.read_json()\n\n\n_display_json(minimal_txt)\n\n{ 'cells': [ {'cell_type': 'markdown', 'metadata': {}, 'source': ['# A minimal notebook']},\n             { 'cell_type': 'code',\n               'execution_count': 1,\n               'metadata': {},\n               'outputs': [{'data': {'text/plain': ['2']}, 'execution_count': 1, 'metadata': {}, 'output_type': 'execute_result'}],\n               'source': ['# Do some arithmetic\\n', '1+1']},\n             {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': []}],\n  'metadata': { 'kernelspec': {'display_name': 'Python 3 (ipykernel)', 'language': 'python', 'name': 'python3'},\n                'language_info': { 'codemirror_mode': {'name': 'ipython', 'version': 3},\n                                   'file_extension': '.py',\n                                   'mimetype': 'text/x-python',\n                                   'name': 'python',\n                                   'nbconvert_exporter': 'python',\n                                   'pygments_lexer': 'ipython3',\n                                   'version': '3.9.5'}},\n  'nbformat': 4,\n  'nbformat_minor': 4}\n\n\nThe important bit for us is the cells:\n\n_display_json(minimal_txt['cells'])\n\n[ {'cell_type': 'markdown', 'metadata': {}, 'source': ['# A minimal notebook']},\n  { 'cell_type': 'code',\n    'execution_count': 1,\n    'metadata': {},\n    'outputs': [{'data': {'text/plain': ['2']}, 'execution_count': 1, 'metadata': {}, 'output_type': 'execute_result'}],\n    'source': ['# Do some arithmetic\\n', '1+1']},\n  {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': []}]\n\n\nThe second cell here is a code cell, however it contains no outputs, because it hasn’t been executed yet. To execute a notebook, we first need to convert it into a format suitable for nbclient (which expects some dict keys to be available as attrs, and some available as regular dict keys). Normally, nbformat is used for this step, but it’s rather slow and inflexible, so we’ll write our own function based on fastcore’s handy dict2obj, which makes all keys available as both attrs and keys.\n\n\n\n\n\n\nNbCell(idx, cell)\n\ndict subclass that also provides access to keys as attrs\n\n\n\nWe use an AttrDict subclass which has some basic functionality for accessing notebook cells.\n\n\n\n\n\n\ndict2nb(js)\n\nConvert dict js to an AttrDict,\n\n\n\nWe can now convert our JSON into this nbclient-compatible format, which pretty prints the source code of cells in notebooks…\n\nminimal = dict2nb(minimal_txt)\nminimal.cells[1]\n\n# Do some arithmetic\n1+1\n\n\n…and is in a format compatible with nbclient to execute it:\n\nnbclient.execute(minimal);\n\nOne nice feature of the output of dict2nb is that we can still use it as a dict, so _display_json still works as before. We can see that the cell has been executed, and the output added back to the nb:\n\ncell = minimal.cells[1]\ncell.outputs\n\n[{'output_type': 'execute_result',\n  'metadata': {},\n  'data': {'text/plain': '2'},\n  'execution_count': 1}]\n\n\nThe abstract syntax tree of source code cells is available in the parsed_ property:\n\ncell.parsed_(), cell.parsed_()[0].value.op\n\n([<ast.Expr at 0x7fe2cc3668b0>], <ast.Add at 0x7fe2d2d29850>)\n\n\nSince loading JSON and converting to an NB is something we’ll do a lot, we’ll create a shortcut function for it:\n\n\n\n\n\n\nread_nb(path)\n\nReturn notebook at path\n\n\n\n\nminimal = read_nb(minimal_fn)\nprint(minimal.cells[0])\n\n{'cell_type': 'markdown', 'metadata': {}, 'source': '# A minimal notebook', 'idx_': 0}\n\n\n\n\n\nnbprocess uses a settings.ini file in the root of the project to store all configuration details. This file is in ConfigParser format, and can be read and written conveniently using fastcore’s Config class.\n\n\n\n\n\n\nnbprocess_create_config(user: str, lib_name: str = None, description='TODO fill me in', author='TODO fill me in', author_email='todo@example.org', path: str = '.', cfg_name: str = 'settings.ini', branch: str = 'master', host: str = 'github', git_url: str = 'https://github.com/%(user)s/%(lib_name)s/tree/%(branch)s/', custom_sidebar: <function bool_arg at 0x7f47b8338700> = False, nbs_path: str = '.', lib_path: str = '%(lib_name)s', doc_path: str = 'docs', tst_flags: str = '', version: str = '0.0.1', keywords='python', license='apache2', copyright='', status='3', min_python='3.6', audience='Developers', language='English')\n\nCreates a new config file for lib_name and user and saves it.\n\n\n\nThis is a wrapper for fastcore’s save_config_file which sets some nbprocess defaults. It is also installed as a CLI command.\n\n\n\n\n\n\nget_config(cfg_name='settings.ini', path=None)\n\nConfig for ini file found in path (defaults to cwd)\n\n\n\nget_config searches for settings.ini in the current directory, and then in all parent directories, stopping when it is found.\n\nnbprocess_create_config('fastai', path='..', nbs_path='nbs', tst_flags='tst', cfg_name='test_settings.ini')\ncfg = get_config('test_settings.ini')\ntest_eq(cfg.lib_name, 'nbprocess')\ntest_eq(cfg.git_url, \"https://github.com/fastai/nbprocess/tree/master/\")\ncwd = Path.cwd()\ntest_eq(cfg.config_path, cwd.parent.absolute())\ntest_eq(cfg.path('lib_path'), cwd.parent/'nbprocess')\ntest_eq(cfg.path('nbs_path'), cwd)\ntest_eq(cfg.path('doc_path'), cwd.parent/'docs')\n\n\n\n\n\n\n\n\n\n\nadd_init(path)\n\nAdd __init__.py in all subdirs of path containing python files if it’s not there already\n\n\n\nPython modules require a __init.py__ file in all directories that are modules. We assume that all directories containing a python file (including in subdirectories of any depth) is a module, and therefore add a __init__.py to each.\n\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    (d/'a/b').mkdir(parents=True)\n    (d/'a/b/f.py').touch()\n    (d/'a/c').mkdir()\n    add_init(d)\n    assert not (d/'a/c'/_init).exists(), \"Should not add init to dir without py file\"\n    for e in [d, d/'a', d/'a/b']: assert (e/_init).exists(),f\"Missing init in {e}\"\n\n\n\n\n\n\n\nwrite_cells(cells, hdr, file, offset=0)\n\nWrite cells to file along with header hdr starting at index offset (mainly for nbprocess internal use)\n\n\n\n\n\n\n\n\n\nbasic_export_nb(fname, name, dest=None)\n\nBasic exporter to bootstrap nbprocess\n\n\n\nThis is a simple exporter with just enough functionality to correctly export this notebook, in order to bootstrap the creation of nbprocess itself.\n\npath = Path('../nbprocess')\n(path/'read.py').unlink(missing_ok=True)\n\nadd_init(path)\nbasic_export_nb(\"01_read.ipynb\", 'read.py')\n\ng = exec_new('from nbprocess import read')\nassert g['read'].add_init\nassert 'add_init' in g['read'].__all__"
  },
  {
    "objectID": "05_sync.html",
    "href": "05_sync.html",
    "title": "nbprocess",
    "section": "",
    "text": "Propagating small changes in the library back to notebooks\n\nThe library is primarily developed in notebooks so any big changes should be made there. But sometimes, it’s easier to fix small bugs or typos in the modules directly. nbprocess_update_lib is the function that will propagate those changes back to the corresponding notebooks. Note that you can’t create new cells or reorder cells with that functionality, so your corrections should remain limited.\n\n\n\n\n\n\nnb2dict(d, k=None)\n\nConvert parsed notebook to dict\n\n\n\nThis returns the exact same dict as is read from the notebook JSON.\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal = read_nb(minimal_fn)\n\nminimal_dict = minimal_fn.read_json()\nassert minimal_dict==nb2dict(minimal)\n\n\n\n\n\n\n\nnb2str(nb)\n\nConvert nb to a str\n\n\n\n\n\n\n\n\n\nwrite_nb(nb, path)\n\nWrite nb to path\n\n\n\nThis returns the exact same string as saved by Jupyter.\n\ntmp = Path('tmp.ipynb')\ntry:\n    minimal_txt = minimal_fn.read_text()\n    write_nb(minimal, tmp)\n    assert minimal_txt==tmp.read_text()\nfinally: tmp.unlink()\n\n\n\n\n\n\n\nabsolute_import(name, fname, level)\n\nUnwarps a relative import in name according to fname\n\n\n\n\ntest_eq(absolute_import('xyz', 'nbprocess', 0), 'xyz')\ntest_eq(absolute_import('', 'nbprocess', 1), 'nbprocess')\ntest_eq(absolute_import('core', 'nbprocess', 1), 'nbprocess.core')\ntest_eq(absolute_import('core', 'nbprocess/vision', 2), 'nbprocess.core')\ntest_eq(absolute_import('transform', 'nbprocess/vision', 1), 'nbprocess.vision.transform')\ntest_eq(absolute_import('notebook.core', 'nbprocess/data', 2), 'nbprocess.notebook.core')\n\n\n\n\n\n\n\nupdate_lib(fname: str)\n\nPropagates any change in the modules matching fname to the notebooks that created them\n\n\n\n\nfrom nbprocess.export import nbs_export\nnbs_export()"
  },
  {
    "objectID": "09_processors.html",
    "href": "09_processors.html",
    "title": "nbprocess",
    "section": "",
    "text": "Some processors for NBProcessor\n\n\n\nOn this page we’ll be using this private helper to process a notebook and return the results, to simplify testing:\n\ndef _run_procs(procs=None, preprocs=None, postprocs=None):\n    nbp = NBProcessor(_test_file, procs, preprocs=preprocs, postprocs=postprocs)\n    nbp.process()\n    return '\\n'.join([str(cell) for cell in nbp.nb.cells])\n\n\n\n\n\n\n\nmk_cell(text, code=True)\n\nCreate a NbCell containing text\n\n\n\n\n\n\n\n\n\ncreate_output(txt, mime)\n\nAdd a cell output containing txt of the mime text MIME sub-type\n\n\n\n\n\n\n\n\n\nCellRunner(name='kernel', glb=None)\n\nA TinyKernel subclass that adds a run method to execute notebook cells\n\n\n\n\n\n\n\n\n\n\n\n\nstrip_ansi(cell)\n\nStrip Ansi Characters.\n\n\n\nGets rid of colors that are streamed from standard out, which can interfere with static site generators:\n\nres = _run_procs(strip_ansi)\nassert not _re_ansi_escape.findall(res)\n\n\n\n\n\n\n\nhide_(nbp, cell)\n\nHide cell from output\n\n\n\n\nres = _run_procs(hide_)\nassert 'you will not be able to see this cell at all either' not in res\n\n\n\n\n\n\n\nhide_line(cell)\n\nHide lines of code in code cells with the directive hide_line at the end of a line of code\n\n\n\n\nres = _run_procs(hide_line)\nassert r\"def show():\\n    a = 2\" in res\n\n\n\n\n\n\n\nfilter_stream_(nbp, cell, *words)\n\nRemove output lines containing any of words in cell stream output\n\n\n\n\nres = _run_procs(filter_stream_)\nexp=r\"'A line\\n', 'Another line.\\n'\"\nassert exp in res\n\n\n\n\n\n\n\nclean_magics(cell)\n\nA preprocessor to remove cell magic commands\n\n\n\n\nres = _run_procs(clean_magics)\nassert \"%%\" not in res\n\n\n\n\n\n\n\nlang_identify(cell)\n\nA preprocessor to identify bash/js/etc cells and mark them appropriately\n\n\n\nWhen we issue a shell command in a notebook with !, we need to change the code-fence from python to bash and remove the !:\n\nres = _run_procs(lang_identify)\nassert \"'language': 'bash'\" in res\n\n\n\n\n\n\n\nrm_header_dash(cell)\n\nRemove headings that end with a dash -\n\n\n\n\nres = _run_procs(rm_header_dash)\nassert 'some words' in res\nassert 'A heading to Hide' not in res\nassert 'Yet another heading to hide' not in res\n\n\n\n\n\n\n\nrm_export(cell)\n\nRemove cells that are exported or hidden\n\n\n\n\nres = _run_procs(rm_export)\nassert 'dontshow' not in res\n\n\n\n\n\n\n\nexec_show_docs()\n\nExecute cells needed for show_docs output, including exported cells and imports\n\n\n\n\nres = _run_procs(exec_show_docs)\n\n\n\n\n\n\n\nclean_show_doc(cell)\n\nRemove ShowDoc input cells\n\n\n\n\n\n\n\n\n\n\n\n\ninsert_warning(nb)\n\nInsert Autogenerated Warning Into Notebook after the first cell.\n\n\n\nThis preprocessor inserts a warning in the markdown destination that the file is autogenerated. This warning is inserted in the second cell so we do not interfere with front matter.\n\nres = _run_procs(preprocs=[insert_warning])\nassert \"<!-- WARNING: THIS FILE WAS AUTOGENERATED!\" in res\n\n\n\n\n\n\n\nadd_show_docs(nb)\n\nAdd show_doc cells after exported cells, unless they are already documented\n\n\n\n\nres = _run_procs(preprocs=add_show_docs)\nassert \"show_doc(some_func)'\" in res\nassert \"show_doc(and_another)'\" in res\nassert \"show_doc(another_func)'\" not in res\n\n\nfrom nbprocess.export import nbs_export\nnbs_export()"
  }
]